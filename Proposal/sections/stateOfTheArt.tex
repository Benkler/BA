

\chapter{State of the Art}
\label{ch:StateOfTheArt}

\section{Literature Review}
\label{sec:StateOfTheArt:LiteratureReview}

\begin{table}[!h]
\centering
     
	
	 \rowcolors{2}{gray!25}{white}
	\begin{tabularx}{\linewidth}{lXXlX}
		\rowcolor{gray!50}
		Link & Titel & Author   & Origin & Search String  \\
		
		\rowcolor{gray!50}
		& & (Year) & & \\
		
		\cite{ExtractionMazlami} & Extraction of Microservices from Monolithic Software Architectures  & G. Matzlami et. al. (2017) & Google Scholar&  {\itshape microservice identification }  \\
		
		
		\cite{ObjectAwareAmiri} & Object-Aware Identification of Microservice & M. J. Amiri (2018) & IEEE & \textit{identification microservices}\\\
		
		\cite{interfaceAnalysisBaresi} & Microservices Identification Through Interface Analysis & L. Baresi et. al. (2017)& google scholar & \textit{microservice identification}\\
		
		
		
		 
		 \cite{FunctionalDecompositionHeinrich}& Identifying Microservices Using Functional Decomposition & S. Tyszberowicz et. al. (2018) & \textit{provided} & \textit{n/a} \\
		 
		 \cite{DomainEngineeringMunezero} & Partitioning Microservices: A Domain Engineering Approach & I. J. Munezero et. al. (2018) & IEEE & \textit{identify microservices}\\
		 
		 
		 \cite{DataflowDrivenChen} & From Monolith to Microservices: A Dataflow-Driven Approach & R.Chen et. al & IEEE & monolith to microservice \\
		 
		\cite{HeuristicsAlwis} & Function-Splitting Heuristics for Discovery of Microservices in Enterprise Systems & A. De Alwis et. al. (2018 )& Google Scholar & identify microservices \\
		
	\cite{ServiceCutter} & 	Service Cutter: A Systematic Approach to Service Decomposition& M. Gysel et. al. (2016) & \cite{interfaceAnalysisBaresi} & \textit{n/a} \\
	\end{tabularx}
	
\end{table}

\pagebreak



\section{Comparison and applicability of the approaches}
\label{sec:StateOfTheArt:ComparisonAndApplicability}
* bedeuted INFO \\
+ beudeted PRO \\
- bedeuted CONTRA \\

\subsection{Extraction of Microservices from Monolithic Software Architectures}
* informal migration patterns exists. Lack of Formal Models \\
* small and recent body of work on how to migrate monolith to MS\\
* clas based extraction model, construct graph, process by clustering algorithm\\
* references Service Cutter (Pros and Cons)\\
* 2 phases: Construction (monolith to graph), clustering (decompose graph to cluster)\\
* starts with code base/repo from VCS\\
* each class is a node, edges have weights according to coupling strategy (classes that are not coupled are discarded)\\
* Logical Coupling Strategy(LC): Single Responsibility principle (Software has only one reason to change), enforce strong module boundaries (concept of MS) --> developers only make changes to the module (found in Change History, Class Files changed together belong togehter) ==> Weight is : for each pair of class look how often they changed together\\
* Semantic Coupling Strategy(SC): each MS correspond to one bounded context (DDD) from domain, examine contents/semantics of source code, term-frequency invese-documents-frequency method (tf-idf), compute relation of two classes regarding domain concepts\\
* tf-idf: Compute scalar vector for each class and compute cosine similarity between pairwise distinct classes\\
* tf-idf: Tokenize class, set of words, filter stop words, compare two classes regarding their common words with tf-idf formula\\
* Main Concern: Well organized teams, cross-functional but also reduce communication overhead to external teams while maximize internal\\
* Contributor Coupling (CC): team/orga info used to recover relationship among sw artifacts ==> Ownership architecture read from VSC history by identifying how many developers worked on the same pair of classes (weight!)\\


* CLustering Algo: Invert weights to favor edges  with heigh weight, Kruskal for MST (calculate remaining edges), sort edges, reverse list, delete first element (originally lowest weight; now first in the list), in each iteration step: DFS(Depth-First) on edgesMST returns number of partitions, iterate and delete while n<nGiven
*Reduce Cluster: after n cluster were formed, method reduceClusters splits up unusually large clusters (due to language and framework, special classes may have extraordinary high coupling) --> Delete those classes until size of node (numbe rof classes seems to be appropriate)
------------------------------------\\


+ algorithmic recommendation of ms candidates implemented in web-based prototype\\
+ unites traditional decomposition techniques and microservice extraction approaches/design principles\\
+ algorithm uses 3 different coupling strategies: Can be combined for better results\\
+ performance of algortihm was satisfying according to author
+ shows significant team size reduction (less than half), given that alls contributirs for given cluster work on a MS
+ no work in advance!!!!

---------------------------------------\\


- rely on (meta-)data extracted from codebase\\
- needs VCS (proper change history)\\
- needs ORM model that models data entities as ordinary classes
- 2 (independent) changes in one commit destroy SRP\\
- SW must have gone through evolution process for LC\\
- Naming of class, methods, attributes needs to reflect domain language to make tf-idf possible\\
- Contributor Coupling: requires more developers\\
- Reduce Clusters: Useful to delete those classes? wat if this is core of application that belongs together
- nPart is given by user: What is the right granularity?\\

--------------------------------------\\
*Conclusion: Beta implementation existing, paper is precise in usage, master-thesis exists (LINK!!!)with detailed information, easy algorithms, good performance, \\
* No Information about right n, not domain oriented, ORM necessary,
BUT: Not applicable to CoCoME as VCS history, software evolution, several contributor necessary



\subsection{Partitioning Microservices:A Domain ENgineering Approach}

* One BusinessCapability determines one MS, based on domain engineering
* Business Capability: Something hat a business does;combined as functionality that have something in common (instead of usin collections of data entities and CRUD operations in them)
* Paper states: Developers struggle to define granulaity of business capability (too small => Communication overhead, to large: heavyweight SOA)
* Appropriate MS size determined by component boundaries
* DDD: Choosing appropriate boundary. MAIN GOAL: systematically group requirements in domain model and implement code  of that
* Design Domain model with sub-domains (each sub-domain has one to more bounded contexts)
(* Sub-Domain is problem space, bounded.context is solution space and maps software artifacts to sub-domains ==> Info aus Internet)
* DDD Patterns: Context Map/Counded Contexts (makes explicit boundaries between domains,), Aggregates (logical boundaries for cluster of domain objects that change during one transaction, ensure consistency among parallel operations that are considered to be one unit in reagrd to change), Ubiquitous Language (ensures that impementation uses the same terms as business), Separation of Entities and Value types (Entities are Objects of DDD, Not all Objects need to be entitites, for value objects identity is not necessary))
* MS should be autonomous (changes not affecting others): One bounded context is one MS (ore more if fine grained needed, BUT never on C in different MS)
* Approach: Defined domain and ubiquitous language ar prerequisites, split into sb domains if domain is too wide, find boundary of each responsibility, make it as business capability (focus on relationship among different services), each capability is a MS, find out relationships between services with domain model and reconfigure if necessary (too many dependencies)


--------------------------------------------\\


+uses DDD. Many papers and experience reports state that this is the way to gto


---------------------------------------------\\

- draw the boundaries is the ky task and requires domain experts
- requires well defined domain, pre-existing ubiquitous language
- "Find the boundary of each responsibility and make it as a business capability" ==> But how? Requires DOmain experts
- Procedure is not concise at all and only explains approach on a top level (half a page)
- Does not define how procedure analyses "loose coupling possible"
- how break down in subdomains?
- uses brainstorming and interaction wit domain experts to identify and define parts of the microservice


---------------------------------------------\\

* Summary: \\
*Hot topic according to other papers (interviews, reports) but this paper lacks on accuracy.  \\
* Simply said: Get some domain-experts and let them cut your domain in different bounded contexts \\
* This is what was already done when migrating CoCoME (probably not following the patterns but having domain experts )

\subsection{From Monolith to Microservices: A Dataflow-Driven Approah}

*top-down dataflow driven decomposition algorithm
* 3 step approach \\
* purified DFD (focus on data's semanteme and operations only, excludes side information), more data focused than traditional \\
* purified DFD (PFD) is a directed Graph with nodes (processing operations nodes and data nodes) and
Approach: \\
* construct traditional DFD (according to business logic extracted from users' natural-language descriptions) \\
* Manual construction of purified dataflow by two rules (more like guidelines)\\
*algorithmic construction (condensing of PDS) of the decomposable Dataflow by applying a set of rules
* combines same operations with same output data
* each operation at the end is a MS


------------------------------------------------------\\

+systematic Algorithm
+ purified DFD represents real information flow of corresponding business logic
+ reduce human mistakes when drawing a composable DFD
+ the 5 rules are easy to understand 

------------------------------------------------------\\
- Semi Automated \\
- traditional DFD constructed on users descriptions and code (detailed data flow) ==> exact? Expertise necessary \\
- or: DFD needs to be existant \\
- transforming traditional to purified is a non trivial task \\
- identifying same data operation requires expertise: Combination of sam operations  based on operation names (what if names are different through the project)
- no implementation \\
- not applied to larger projects
- based on user's natural language: semantic verbs correspond activities (later a MS) \\
-really really fine-grained MS, based on single operation (most fine graines as possible) \\
- no domain information included\\
- what about get/create....   hier weiß ich nicht, wie ich beschreiben soll dass es nicht geht! \\
- zweites beispiel zigt das deutlicher mit der operation QUERY \\


------------------------------------------------------\\

Conclusion \\

TODO: Why not applicable to CoCoME

\subsection(Function-Splitting Heuristics for Discovery of Microservices in Enterprise Systems)

* function splitting based on i) Object subtypes (lowest granularity of sw based on behavioural properties) and ii) common execution fragments across software (lowest ... based on structural properties) \\
* authors say: MS not adopted for enterprise systems (ERP/CRM Sw which is large and has complex business process) \\
* lot of BO's (business objects) with many-to-many relationship, functional dependencies
* key strumbling block: "limited insight from syntactic structures of code for profiling software dependencies and identifying the semantics available through the BO relationships.
* Process:\\
* ES system with modules that have function which execute basic CRUD ops on BOs (centralized DB) vs. own DB in MS (structural difference)
* Behavioural property: Based on invocation of properties in well-defined processing sequences (different execution sequences reflect a set of SESE regions   single-entry single-exit) \\
*Enterprise System as finite automaton with operations as labels: Vertices defined by states and relation defined by transistion functions \\
* SESE-Fragments: Induces a function (SubGraph of ES), Function is set of operations \\
* behaviour (of both MS and ES) based on invocation of operations --> Analyse those processng sequences \\
*Heuristics: \\
* given: several similar Execution paths with changed BOs, only Attributes they use are different (structural splitting of objects at BO level)
* i) Subtype Relation in SESE(S): Subtype between callgraphs exists if ops, input/output of child are present in parent AND 80percent of parent states appear in child \\
* given: execution pattern occurs in several patterns (depend on functional relationship): A always before B ==> "as a relationship" property \\
* ii) Commonality: two call graphs have common sub-graph ==> Small Subgraphs (small MS), large Subgrapgh (large MS)\\
*Process:\\
* two components: Business Object Analyser (BOA), System Dynamic Analyser(SDA) \\
* BOA: Two models, one for evaluating the SQL queries to identify relationships between DB tables and one for identifying BOs based on the relationships and data similarities ==> Input for SDA \\
*SDA: BOs and callgraphs as Input, identifies Frequent Execution Patterns(FEP) in provided SESEs(S)  by using clustering algorithms\\
*FEP is evaluated against heuristics and classified in categories \\
* categorized patterns evaluated by BO Relationship Analysis and SESE derivation model  \\
* Microservice Recommendation Interface (MRI) provides MS recommendations \\
* SDA+MRI: two algos i) set of subgraphs in given set of callgraphs ii) analyse subgraphs for SESE code fragments (functions) that are related to same BO



------------------------------------------------------\\
+ prototype implementation exists
+ already conducted two experiments 


------------------------------------------------------\\
- incremental process: most prominent components extracted and reimplemented as MS
- really complex
- BOA not given in paper
- uses a lot of algo that are not explained
- First experiment: log data analysed with DISCO to generate call graphs

------------------------------------------------------\\
Coclusion: \\
* Approach not fully implemented, implementation not available
* Complex algorithms  that use other algorithms from other papers
* log data to get call graphs
* We do not want to incrementally extract MS
* Experiments focused on speed/ressources/time per requests ==> Did not focus on finding the "right" MS according to MS principles
* Nopt applicable to CoCoME





